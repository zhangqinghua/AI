Q-learning算法让Flappy Bird自己学习怎么飞
=========


## 主要内容
> #### Flappy Bird自己学习怎么飞
>> #### 问题分析
>> #### 状态的选择
>> #### 动作的选择
>> #### 关于Q 
>> #### 训练
> #### 代码实现


## Flappy Bird自己学习怎么飞


### 1. *问题分析*
让小鸟学习怎么飞是一个**强化学习**的过程，强化学习中有**状态**、**动作**、**奖赏**这三个要素。**智能体**（这里就是指我们聪明的小鸟）需要根据当前状态来采取动作，获得相应的奖赏之后，再去改进这些动作，使得下次再到相同状态时，智能体能做出更优的动作。


### 2. *状态的选择*
在这个问题中，状态的提取方式可以有很多种：比如说取整个游戏画面做图像处理啊，或是根据小鸟的高度和管子的距离啊。
在这里选用的是跟SarvagyaVaish项目相同的状态提取方式，
即取小鸟到下一根下侧管子的水平距离和垂直距离差作为小鸟的状态![(dx, dy)](https://www.zhihu.com/equation?tex=(dx,dy))，
![dx](https://www.zhihu.com/equation?tex=dx)为水平距离，![dx](https://www.zhihu.com/equation?tex=dy)为垂直距离。  
![test](https://pic4.zhimg.com/90f9c09449f9a948107715bc1a8fb03b_b.png)  


### 3. *动作的选择*
小鸟只有两种动作可选：1.向上飞一下，2.什么都不做。


### 4. *奖赏的选择*
这里采用的方式是：小鸟活着时，每一帧给予1的奖赏；若死亡，则给予-1000的奖赏；若成功经过一个水管，则给予50的奖赏。


### 5. *关于Q*
**Q**为**动作效用函数**，用于评价在特定状态下采取某个动作的优劣，可以将之理解为**智能体**的大脑。我们可以把**Q**当做是一张表。表中的每一行是一个状态，每一列（这个问题中共有两列）表示一个动作（飞与不飞）。

| 状态 | 飞 | 不飞 |
| :------:| :------: | :------: |
| ![](https://www.zhihu.com/equation?tex=dx_{1},dy_{1}) | 1 | 3 |
| ![](https://www.zhihu.com/equation?tex=dx_{1},dy_{2}) | 3 | 4 |
| ... | ... | ... |
| ![](https://www.zhihu.com/equation?tex=dx_{n},dy_{m-1}) | 2 | 1 |
| ![](https://www.zhihu.com/equation?tex=dx_{n},dy_{m}) | -100 | 1 |  


这张表一共 ![](https://www.zhihu.com/equation?tex=m\times%20n) 行，表示 ![](https://www.zhihu.com/equation?tex=m\times%20n) 个状态，每个状态所对应的动作都有一个**效用值**。训练之后的小鸟在某个位置处飞与不飞的决策就是通过这张表确定的。小鸟会先去根据当前所在位置查找到对应的行，然后再比较两列的值（飞与不飞）的大小，**选择值较大的动作作为当前帧的动作**。


### 6. *训练*
那么这个Q是怎么训练得来的呢，贴一段伪代码。  
```
Initialize Q arbitrarily //随机初始化Q值
Repeat (for each episode): //每一次游戏，从小鸟出生到死亡是一个episode
    Initialize S //小鸟刚开始飞，S为初始位置的状态
    Repeat (for each step of episode):
        根据当前Q和位置S，使用一种策略，得到动作A //这个策略可以是ε-greedy等
        做了动作A，小鸟到达新的位置S'，并获得奖励R //奖励可以是1，50或者-1000
        Q(S,A) ← (1-α)*Q(S,A) + α*[R + γ*maxQ(S',a)] //在Q中更新S
        S ← S'
    until S is terminal //即到小鸟死亡为止
```
